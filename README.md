# ğŸ§  Document Categorization using Knowledge Distillation (BERT â†’ Custom CNN)

This project demonstrates an efficient **document categorization pipeline** using a **knowledge distillation** approach, where a high-capacity **BERT teacher model** guides a lightweight **custom CNN student model**. Our goal is to retain most of BERTâ€™s accuracy while significantly reducing model size and inference timeâ€”making the solution deployment-friendly.

---

## ğŸš€ Overview

- **Task**: Multi-class document classification
- **Teacher Model**: Pre-trained BERT (fine-tuned on the task)
- **Student Model**: Lightweight CNN architecture
- **Distillation Technique**: Custom distillation loss combining soft targets and hard labels
- **Motivation**: Deploy BERT-level accuracy in low-resource environments using a smaller student model

---

## ğŸ“ Project Structure

```
document-distillation/
â”‚
â”œâ”€â”€ data/                      # Dataset files and preprocessing scripts
â”‚   â””â”€â”€ preprocess.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ teacher_bert.py        # BERT fine-tuning model
â”‚   â””â”€â”€ student_cnn.py         # Custom CNN model
â”‚
â”œâ”€â”€ distillation/
â”‚   â””â”€â”€ loss.py                # Custom distillation loss implementation
â”‚
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train_teacher.py       # Fine-tune the teacher BERT
â”‚   â”œâ”€â”€ train_student.py       # Train student with distillation
â”‚   â””â”€â”€ utils.py               # Utilities for training, logging, evaluation
â”‚
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸ“Š Methodology

### ğŸ”¹ Teacher Model: BERT
- Uses `bert-base-uncased` from HuggingFace Transformers
- Fine-tuned on the classification dataset
- Provides both **hard labels** and **soft logits** as supervision

### ğŸ”¹ Student Model: Custom CNN
- A simple CNN with:
  - Embedding layer
  - Multiple convolutional filters
  - Global max pooling
  - Fully connected output layer
- Extremely parameter-efficient

### ğŸ”¹ Custom Distillation Loss

The loss function blends:

```
Total Loss = Î± * CrossEntropy(student_logits, true_labels) + 
             (1 - Î±) * KLDiv(student_logits, teacher_logits / T)
```

Where:
- `Î±` is the weight balancing between distillation and classification loss
- `T` is the temperature scaling factor
- `KLDiv` encourages the student to mimic the softened output distribution of the teacher

---

## ğŸ”§ Setup Instructions

```bash
# Clone the repository
git clone https://github.com/yourusername/document-distillation.git
cd document-distillation

# Create a virtual environment
python -m venv venv
source venv/bin/activate   # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ“ˆ Training

### 1. Train the BERT Teacher
```bash
python train/train_teacher.py
```

### 2. Train the CNN Student using Distillation
```bash
python train/train_student.py
```

Optional flags:
- `--alpha`: weight for cross-entropy loss
- `--temperature`: temperature for softening logits

---

## ğŸ§ª Evaluation

Both training scripts provide accuracy, precision, recall, and F1-score after each epoch. You can also run standalone evaluation:

```bash
python train/eval.py --model student
```

---

## ğŸ§  Key Benefits

- ğŸ” **90%+ compression** in model size
- âš¡ **Faster inference** suitable for edge devices
- ğŸ“š Retains most of the teacherâ€™s accuracy with fewer resources
- ğŸ§ª Fully customizable distillation pipeline

---

## ğŸ“Œ TODO

- [ ] Add support for different student architectures (LSTM, Transformer Lite)
- [ ] Integrate TensorBoard for training visualizations
- [ ] Export student model to ONNX / TensorRT for deployment

---

## ğŸ“š References

- [Distilling the Knowledge in a Neural Network (Hinton et al., 2015)](https://arxiv.org/abs/1503.02531)
- [HuggingFace Transformers](https://huggingface.co/transformers/)
- [TextCNN (Yoon Kim, 2014)](https://arxiv.org/abs/1408.5882)

---

## ğŸ’¡ Citation

If this project helped you, consider citing it or giving it a â­ï¸ on GitHub!
